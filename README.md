# üöÄ Efficient LLM Processing and Fine-Tuning

## üß† Overview
This project explores how **large language models can be trained and deployed efficiently on limited hardware**‚Äîbalancing **memory, speed, and accuracy**.  
I built a full experimental pipeline around **Meta‚Äôs LLaMA 3.2 1B** model to evaluate optimization methods like **KV caching, LoRA, gradient accumulation, mixed precision, and activation checkpointing**.

> üí° The goal: make billion-parameter models *trainable and deployable* on a single 16 GB GPU without losing performance.

## üß© Motivation
Most LLM work assumes unlimited compute. I wanted to prove that *careful systems engineering*‚Äînot bigger clusters‚Äîunlocks scalability.  
This project merges my interests in **ML systems optimization** (Adobe, Apple AIML) and **efficient model design** from my published research on *LLM-based music recommendation* (ACM RecSys 2024).

## ‚öôÔ∏è Key Contributions
| Area | Summary | Impact |
|------|----------|--------|
| **KV Cache Profiling** | Removed and re-implemented caching in LLaMA to analyze attention bottlenecks | Quantified a 6.7√ó runtime gap between cached and cache-free inference |
| **LoRA Implementation** | Added low-rank adapters to Q/V projection layers with r = 16, Œ± = 32 | Reduced trainable parameters ‚Üí 0.2 % of full model |
| **Mixed Precision + AMP** | Integrated FP16/FP32 hybrid training with dynamic loss scaling | ~1.8√ó runtime speedup, ~50 % less memory |
| **Gradient Accumulation** | Simulated large-batch updates under memory limits | Stable training on < 16 GB VRAM |
| **Checkpointing** | Applied selective activation checkpointing for transformer blocks | Freed ~40 % activation memory |

## ‚öôÔ∏è System Architecture
efficient-llm/
‚îú‚îÄ‚îÄ model/
‚îÇ ‚îú‚îÄ‚îÄ llama.py # Transformer core
‚îÇ ‚îú‚îÄ‚îÄ lora.py # Custom LoRA linear modules
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ ‚îú‚îÄ‚îÄ inference.py # KV cache experiments
‚îÇ ‚îú‚îÄ‚îÄ finetuning.py # Instruction tuning workflow
‚îÇ ‚îú‚îÄ‚îÄ benchmark_inference.py# Memory/runtime profiling
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îî‚îÄ‚îÄ alpaca_subset.json # 200-sample instruction dataset
‚îî‚îÄ‚îÄ README.md

## ‚ö° Inference Benchmarks

| **Batch** | **Cache** | **Peak Memory (GB)** | **Runtime (s)** | **Œî Runtime** |
|-----------:|:----------:|:--------------------:|:---------------:|:-------------:|
| 1 | ‚úÖ ON | 3.07 | 0.37 | ‚Äî |
| 1 | ‚ùå OFF | 3.23 | 0.41 | +11 % |
| 8 | ‚úÖ ON | 4.50 | 0.52 | ‚Äî |
| 8 | ‚ùå OFF | 5.76 | 1.80 | +246 % |
| 16 | ‚úÖ ON | 6.13 | 0.63 | ‚Äî |
| 16 | ‚ùå OFF | 8.64 | 4.25 | +574 % |

> **Insight:** KV caching is essential for scalable inference‚Äîwithout it, attention recomputation scales linearly with prompt length √ó batch size.

## üéØ Fine-Tuning Results

- **Dataset:** Alpaca subset (200 samples)  
- **Hardware:** NVIDIA P100 (16 GB VRAM)  
- **Optimizer:** SGD (lr = 1e-5, accum = 8)  
- **LoRA Config:** r = 16, Œ± = 32, dropout = 0.05  

| Technique | Peak Mem (MB) | Runtime/Step (s) | Notes |
|------------|---------------|------------------|-------|
| Baseline (FP32) | 14800 | 1.12 | Full-precision fine-tuning |
| + Mixed Precision | 7800 | 0.61 | 2√ó faster |
| + Checkpointing | 6200 | 0.73 | 40 % less VRAM |
| + LoRA (PEFT) | 6100 | 0.68 | Only 0.2 % params trainable |

Loss curves consistently decreased ‚Üí confirmed correct gradient flow and numerical stability.

## üß© Insights
- **Compute ‚Üî Memory Trade-off:** Activation checkpointing and gradient accumulation are complementary; together they make billion-parameter training feasible.  
- **LoRA Generalization:** Preserves base-model knowledge while adapting quickly to new tasks.  
- **Mixed Precision Reliability:** AMP maintained stability across all configurations without underflow.  
- **Scalability:** Single-GPU runs match multi-GPU setups in efficiency per TFLOP when properly tuned.

## üõ†Ô∏è Tech Stack
**Languages:** Python, CUDA  
**Frameworks:** PyTorch (AMP, Checkpointing), LoRA (PEFT)  
**Hardware:** NVIDIA P100 (16 GB)  
**Dataset:** Stanford Alpaca subset  
**Model:** Meta LLaMA 3.2 1B (decoder-only transformer)  

## üìà Future Directions
- Extend to **quantization-aware training (QAT)** for 4-bit fine-tuning  
- Profile **attention kernel fusion** and **Flash-Attention 2** on larger LLaMA variants  
- Integrate **RLHF or DPO** for alignment-style fine-tuning  
- Build a **web demo** for side-by-side inference comparison (cached vs non-cached)
